# config1.yaml

epochs: 100               # int: Number of training epochs (e.g., 10, 40, 100)
batch_size: 32           # int: Batch size (e.g., 4, 8, 16, 32, 64)
lr: 0.0001               # float: Learning rate (e.g., 0.001, 0.0001, 0.003, etc.)
load: null               # str or null: Path to checkpoint to load, or null for training from scratch
scale: 0.5               # float: Image scaling factor (e.g., 1.0, 0.5, 0.25)
val: 0.2                 # float: Validation split (0.0-1.0, e.g., 0.1 = 10%, 0.2 = 20%)
amp: false               # bool: Mixed precision training (true/false)
bilinear: true           # bool: Use bilinear upsampling (true) or transposed conv (false)
classes: 5               # int: Number of output segmentation classes (usually 1 for binary, 2+ for multi-class)
model: unet-convnext-attention      # str: Model type ("unet", "unet-residual", "unet-attention", "unet-convnext-attention")
# encoder: convnext_tiny              # str: Encoder backbone ("convnext_tiny", "convnext_base", etc.)
pretrained: true                    # bool: Use ImageNet pretrained weights for encoder (true/false)
scheduler: onecycle     # str: LR scheduler ("reduce_lr", "cosine", "cosine_restart", "step", "exp", "onecycle")
optimizer: adam          # str: Optimizer ("adam", "adamw", "sgd", "rmsprop", "adabelief", "adagrad")
# Recommended starting learning rates (lr):
#   adam:      0.0001 or 0.001 (most common: 0.0001 for segmentation, can try 0.001)
#   adamw:     0.0001 or 0.001 (similar to Adam, but regularizes better)
#   sgd:       0.01 or 0.1 (with momentum, 0.01 is often safer for segmentation, try 0.1 only with lr scheduler)
#   rmsprop:   0.0005 or 0.001 (can be slightly higher than Adam, but not too much)
#   adabelief: 0.001 (if used, works well with the same as Adam)
#   adagrad:   0.01 (often higher, but usually 0.01 to 0.05 is good, test lower if unstable)
init: none  # str: Weight initialization ("xavier", "kaiming", "orthogonal", "normal", "none" for no init)
t_max: 40                # int: T_max for CosineAnnealingLR (epochs for a full cosine cycle)
eta_min: 1e-6            # float: Minimum LR for CosineAnnealingLR (e.g., 1e-6)
weight_decay: 1e-8       # float: Weight decay (e.g., 0.0, 1e-5, 1e-8)
momentum: 0.999          # float: Momentum for optimizers that support it (e.g., 0.9, 0.99, 0.999)