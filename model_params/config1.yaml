# config1.yaml

epochs: 40               # int: Number of training epochs (e.g., 10, 40, 100)
batch_size: 32           # int: Batch size (e.g., 4, 8, 16, 32, 64)
lr: 0.0001               # float: Learning rate (e.g., 0.001, 0.0001, 0.003, etc.)
load: null               # str or null: Path to checkpoint to load, or null for training from scratch
scale: 0.5               # float: Image scaling factor (e.g., 1.0, 0.5, 0.25)
val: 0.2                 # float: Validation split (0.0-1.0, e.g., 0.1 = 10%, 0.2 = 20%)
amp: false               # bool: Mixed precision training (true/false)
bilinear: true           # bool: Use bilinear upsampling (true) or transposed conv (false)
classes: 5               # int: Number of output segmentation classes (usually 1 for binary, 2+ for multi-class)
model: unet-attention    # str: Model type ("unet", "unet-residual", "unet-attention")
scheduler: cosine        # str: LR scheduler ("reduce_lr", "cosine", "cosine_restart", "step", "exp", "onecycle")
t_max: 40                # int: T_max for CosineAnnealingLR (epochs for a full cosine cycle)
eta_min: 1e-6            # float: Minimum LR for CosineAnnealingLR (e.g., 1e-6)
weight_decay: 1e-8       # float: Weight decay (e.g., 0.0, 1e-5, 1e-8)
momentum: 0.999          # float: Momentum for optimizers that support it (e.g., 0.9, 0.99, 0.999)